{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Process for Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import pHash\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "pd.set_option('max_colwidth',500)\n",
    "pd.set_option('max_rows', 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Import JSON to Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filename = '/Users/cbopp/Data/Nepal/nepal-earthquake-4-27-2248-head-1k.json'\n",
    "#filename = '/Users/cbopp/Data/Nepal/nepal-earthquake-4-27-2248-head-25.json'\n",
    "filename = '/Users/cbopp/Data/Sandy_Image/2012_Hurricane_Sandy-2k.jsonl'\n",
    "l = []\n",
    "\n",
    "for line in open(filename):\n",
    "    l.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(l)\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], format=\"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "df['created_at_epoch'] = df['created_at'].astype('int64')\n",
    "df['text'] = df['text'].apply(lambda s: s.replace('\\n', ' '))\n",
    "df['text'] = df['text'].apply(lambda s: s.replace('\\r', ' '))\n",
    "df['text'] = df['text'].apply(lambda s: s.replace('\\t', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retweeted_status_data = {'tweet_id_str': [], 'rt': [], 'original_tweet_created_at': [], 'original_tweet_id_str': [], 'original_user_created_at': [], 'original_user_id_str': [], 'original_user_default_avatar': [], 'original_user_followers_count': [], 'original_user_friends_count': [], 'original_user_listed_count': [], 'original_user_name': [], 'original_user_tweet_count': [], 'original_user_verified': [], 'original_user_description': []}\n",
    "\n",
    "for tweet in l:  \n",
    "    try:\n",
    "        tweet['retweeted_status']\n",
    "        rt = True\n",
    "    except:\n",
    "        rt = False\n",
    "        \n",
    "    if rt:\n",
    "        retweeted_status_data['tweet_id_str'].append(tweet['id_str'])\n",
    "        retweeted_status_data['rt'].append(rt)\n",
    "        retweeted_status_data['original_tweet_created_at'].append(tweet['retweeted_status']['created_at'])\n",
    "        retweeted_status_data['original_tweet_id_str'].append(tweet['retweeted_status']['id_str'])\n",
    "        retweeted_status_data['original_user_created_at'].append(tweet['retweeted_status']['user']['created_at'])\n",
    "        retweeted_status_data['original_user_id_str'].append(tweet['retweeted_status']['user']['id_str'])\n",
    "        retweeted_status_data['original_user_default_avatar'].append(tweet['retweeted_status']['user']['default_profile_image'])\n",
    "        retweeted_status_data['original_user_followers_count'].append(tweet['retweeted_status']['user']['followers_count'])\n",
    "        retweeted_status_data['original_user_friends_count'].append(tweet['retweeted_status']['user']['friends_count'])\n",
    "        retweeted_status_data['original_user_listed_count'].append(tweet['retweeted_status']['user']['listed_count'])\n",
    "        retweeted_status_data['original_user_name'].append(tweet['retweeted_status']['user']['name'])\n",
    "        retweeted_status_data['original_user_tweet_count'].append(tweet['retweeted_status']['user']['statuses_count'])\n",
    "        retweeted_status_data['original_user_verified'].append(tweet['retweeted_status']['user']['verified'])\n",
    "        try:\n",
    "            retweeted_status_data['original_user_description'].append(tweet['retweeted_status']['user']['description'])\n",
    "        except:\n",
    "            retweeted_status_data['original_user_description'].append(np.NaN)\n",
    "    else:\n",
    "        retweeted_status_data['tweet_id_str'].append(tweet['id_str'])\n",
    "        retweeted_status_data['rt'].append(rt)\n",
    "        retweeted_status_data['original_tweet_created_at'].append(np.NaN)\n",
    "        retweeted_status_data['original_tweet_id_str'].append(np.NaN)\n",
    "        retweeted_status_data['original_user_created_at'].append(np.NaN)\n",
    "        retweeted_status_data['original_user_id_str'].append(np.NaN)\n",
    "        retweeted_status_data['original_user_default_avatar'].append(np.NaN)\n",
    "        retweeted_status_data['original_user_followers_count'].append(np.NaN)\n",
    "        retweeted_status_data['original_user_friends_count'].append(np.NaN)\n",
    "        retweeted_status_data['original_user_listed_count'].append(np.NaN)\n",
    "        retweeted_status_data['original_user_name'].append(np.NaN)\n",
    "        retweeted_status_data['original_user_tweet_count'].append(np.NaN)\n",
    "        retweeted_status_data['original_user_verified'].append(np.NaN)\n",
    "        retweeted_status_data['original_user_description'].append(np.NaN)\n",
    "\n",
    "retweeted_status = pd.DataFrame(retweeted_status_data, columns=['tweet_id_str', 'rt', 'original_tweet_created_at', 'original_tweet_id_str', 'original_user_created_at', 'original_user_id_str', 'original_user_default_avatar', 'original_user_followers_count', 'original_user_friends_count', 'original_user_listed_count', 'original_user_name', 'original_user_tweet_count', 'original_user_verified', 'original_user_description'])\n",
    "\n",
    "retweeted_status['original_user_description'].replace('', np.NaN, inplace=True)\n",
    "\n",
    "retweeted_status['original_tweet_created_at'] = pd.to_datetime(retweeted_status['original_tweet_created_at'], format=\"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "retweeted_status['original_user_created_at'] = pd.to_datetime(retweeted_status['original_user_created_at'], format=\"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "retweeted_status['original_tweet_created_at_epoch'] = retweeted_status['original_tweet_created_at'].astype('int64')\n",
    "retweeted_status['original_user_created_at_epoch'] = retweeted_status['original_user_created_at'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_data = {'tweet_id_str': [], 'user_created_at': [], 'user_id_str': [], 'user_default_avatar': [], 'user_followers_count': [], 'user_friends_count': [], 'user_listed_count': [], 'user_name': [], 'user_tweet_count': [], 'user_verified': [], 'user_description': []}\n",
    "\n",
    "for tweet in l:\n",
    "    user_data['tweet_id_str'].append(tweet['id_str'])\n",
    "    user_data['user_created_at'].append(tweet['user']['created_at'])\n",
    "    user_data['user_id_str'].append(tweet['user']['id_str'])\n",
    "    user_data['user_default_avatar'].append(tweet['user']['default_profile_image'])\n",
    "    user_data['user_followers_count'].append(tweet['user']['followers_count'])\n",
    "    user_data['user_friends_count'].append(tweet['user']['friends_count'])\n",
    "    user_data['user_listed_count'].append(tweet['user']['listed_count'])\n",
    "    user_data['user_name'].append(tweet['user']['name'])\n",
    "    user_data['user_tweet_count'].append(tweet['user']['statuses_count'])\n",
    "    user_data['user_verified'].append(tweet['user']['verified'])\n",
    "    try:\n",
    "        user_data['user_description'].append(tweet['user']['description'])\n",
    "    except:\n",
    "        user_data['user_description'].append(np.NaN)\n",
    "        \n",
    "user = pd.DataFrame(user_data, columns=['tweet_id_str', 'user_created_at', 'user_id_str', 'user_default_avatar', 'user_followers_count', 'user_friends_count', 'user_listed_count', 'user_name', 'user_tweet_count', 'user_verified', 'user_description'])\n",
    "\n",
    "user['user_description'].replace('', np.NaN, inplace=True)\n",
    "user['user_created_at'] = pd.to_datetime(user['user_created_at'], format=\"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "user['user_created_at_epoch'] = user['user_created_at'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Identify Tweets that Contain Extended Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embedded_images(e):\n",
    "    l = []\n",
    "    try:\n",
    "        for o in e['media']:\n",
    "            url = o['media_url']\n",
    "            l.append(url)\n",
    "        return l\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df['e_media'] = df['entities'].apply(embedded_images)\n",
    "df['contains_image_by_e'] = df.e_media.apply(lambda o: type(o) == list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_media_data = {'tweet_id_str': [], 'media_url': [], 'image_id_str': [], 'image_original_tweet_id_str': []}\n",
    "\n",
    "for tweet in l:\n",
    "    for entity in tweet['entities']:\n",
    "        if entity == 'media':\n",
    "            for image in tweet['entities'][entity]:\n",
    "                e_media_data['tweet_id_str'].append(tweet['id_str'])\n",
    "                e_media_data['media_url'].append(image['media_url'])\n",
    "                e_media_data['image_id_str'].append(image['id_str'])\n",
    "                try:\n",
    "                    e_media_data['image_original_tweet_id_str'].append(image['source_status_id_str'])\n",
    "                except:\n",
    "                    e_media_data['image_original_tweet_id_str'].append(np.NaN)\n",
    "\n",
    "e_media = pd.DataFrame(e_media_data, columns=['tweet_id_str', 'image_original_tweet_id_str', 'image_id_str', 'media_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For newer Twitter data sets that contain the extended_entities object\n",
    "def embedded_images(ee):\n",
    "    l = []\n",
    "    for o in ee['media']:\n",
    "        url = o['media_url']\n",
    "        l.append(url)\n",
    "    return l\n",
    "\n",
    "df['ee_media'] = df[df.extended_entities.notnull()]['extended_entities'].apply(embedded_images)\n",
    "df['contains_image_by_ee'] = df.ee_media.apply(lambda o: type(o) == list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Unwind URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['urls'] = df.entities.apply(lambda u: u['urls'])\n",
    "df['url_count'] = df['urls'].apply(lambda l: len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link_shorteners = ['bit.ly','goo.gl','fb.me','is.gd'] #TODO: Add more\n",
    "\n",
    "def resolve_urls(ent):\n",
    "    l = []\n",
    "    session = requests.Session()\n",
    "    session.mount(\"http://\", requests.adapters.HTTPAdapter(max_retries=2))\n",
    "    session.mount(\"https://\", requests.adapters.HTTPAdapter(max_retries=2))\n",
    "    \n",
    "    for o in ent['urls']:\n",
    "        url = urlparse(o['expanded_url'])\n",
    "        if url.netloc in link_shorteners:\n",
    "            try:\n",
    "                l.append(session.get(url=url.geturl(),timeout=6).url)\n",
    "                #TODO: If the connection fails, the URL doesn't get appended. Fix to append anyway.\n",
    "                #TODO: Should this be a dict instead of original_url: expanded_url?\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            l.append(url.geturl())\n",
    "    return l\n",
    "\n",
    "df['expanded_urls'] = df.entities.apply(resolve_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_search = 'blog.meconfess.com'\n",
    "#to_search = 'www.myopenforum.com'\n",
    "#to_search = 'findmejob.org'\n",
    "#to_search = 'www.laopinion.com'\n",
    "#to_search = 'visibli.com'\n",
    "#to_search = 'buzzfeed'\n",
    "df[df.expanded_urls.str[0].str.contains(to_search, case=False, na=False)]\n",
    "\n",
    "df[df.expanded_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df.expanded_urls.str[1].str.contains(to_search, case=False, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Identify Images by HTTP Content Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def content_type(urls):\n",
    "    l = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(url)\n",
    "            sys.stdout.flush()\n",
    "            r = requests.get(url, timeout=0.1)\n",
    "            l.append(r.headers['Content-Type'].split(sep=';')[0])\n",
    "        except:\n",
    "            pass #TODO: Make this less horrible.\n",
    "    return l\n",
    "\n",
    "df['content_type'] = df.expanded_urls.apply(content_type)\n",
    "df['contains_image_by_ct'] = df.content_type.apply(lambda t: ('image/jpeg' or 'image/gif') in t) #TODO: Add more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Identify Images by File Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extensions(urls):\n",
    "    l = []\n",
    "    for url in urls:\n",
    "        if url[-4:-3] == \".\":\n",
    "            s = url[-3:].lower()\n",
    "            if s.isalpha():\n",
    "                l.append(s)\n",
    "        elif url[-5:-4] == \".\":\n",
    "            s = url[-4:].lower()\n",
    "            if s.isalpha():\n",
    "                l.append(s)\n",
    "    return l\n",
    "\n",
    "df['extensions'] = df.expanded_urls.apply(extensions)\n",
    "df['contains_image_by_ex'] = df.extensions.apply(lambda t: ('jpg' or 'jpeg' or 'gif' or 'png' or 'svg' or 'bmp') in t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compare Results of Image Identification Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['contains_image_by_ct'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['contains_image_by_ex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['contains_image_by_e'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['contains_image_by_ee'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###TODO: Deal with Photo Sites like Instagram and Flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "photo_sites = ['instagram.com'] #TODO: Add more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compute Perceptual Hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hash1 = pHash.imagehash('/Users/cbopp/Desktop/testing_images/image3.jpg')\n",
    "hash2 = pHash.imagehash('/Users/cbopp/Desktop/testing_images/image4.jpg')\n",
    "print 'Hamming distance:',pHash.hamming_distance(hash1, hash2),'(',hash1,'/',hash2,')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '%08x' % (hash1)\n",
    "print '%08x' % (hash2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '%d' % (pHash.hamming_distance(hash1, hash2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digest1 = pHash.image_digest('/Users/cbopp/Desktop/image3.jpg',1.0,1.0,180)\n",
    "digest2 = pHash.image_digest('/Users/cbopp/Desktop/image4.jpg',1.0,1.0,180)\n",
    "print 'Cross-correlation:',pHash.crosscorr(digest1,digest2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Examine Retweet Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference http://0-proquestcombo.safaribooksonline.com.libraries.colorado.edu/book/web-development/twitter/9781449303624/1dot-the-recipes/id2867843?uicode=ucb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.to_pickle('/Users/cbopp/Data/Nepal/nepal-earthquake-4-27-2248-head-1k.pkl'\n",
    "#df.to_pickle('/Users/cbopp/Data/Nepal/nepal-earthquake-4-27-2248-head-25.pkl'\n",
    "df.to_pickle('/Users/cbopp/Data/Sandy_Image/2012_Hurricane_Sandy-2k.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/Users/cbopp/Data/Sandy_Image/2012_Hurricane_Sandy-2k.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Get Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_directory = '/Users/cbopp/Data/Sandy_Image/images/'\n",
    "\n",
    "def download_images(l):\n",
    "    filenames = []\n",
    "    \n",
    "    for url_original in l:\n",
    "\n",
    "        u_original = urlparse(url_original)\n",
    "        \n",
    "        if u_original.netloc == \"p.twimg.com\":\n",
    "            url = \"http://pbs.twimg.com/media\" + u_original.path\n",
    "        else:\n",
    "            url = url_original\n",
    "        \n",
    "        u = urlparse(url)\n",
    "        filename = (u.netloc + u.path).replace(\"/\",\"_\")\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(image_directory + filename, 'wb') as out_file:\n",
    "                shutil.copyfileobj(response.raw, out_file)\n",
    "            filenames.append(filename)\n",
    "        else:\n",
    "            print(\"Connection Error: {}\".format(url))\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "df['e_files'] = df['e_media'][df['contains_image_by_e'] == True].apply(download_images)\n",
    "df['ee_files'] = df['ee_media'][df['contains_image_by_ee'] == True].apply(download_images)\n",
    "df['ex_files'] = df['expanded_urls'][df['contains_image_by_ex'] == True].apply(download_images)\n",
    "df['ct_files'] = df['expanded_urls'][df['contains_image_by_ct'] == True].apply(download_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_directory = '/Users/cbopp/Data/Sandy_Image/images/'\n",
    "\n",
    "def download_images(row):\n",
    "    \n",
    "    f_df = pd.DataFrame\n",
    "    \n",
    "    for _, v in row['e_media'].iteritems():\n",
    "    \n",
    "    for url_original in l:\n",
    "\n",
    "        u_original = urlparse(url_original)\n",
    "        \n",
    "        if u_original.netloc == \"p.twimg.com\":\n",
    "            url = \"http://pbs.twimg.com/media\" + u_original.path\n",
    "        else:\n",
    "            url = url_original\n",
    "        \n",
    "        u = urlparse(url)\n",
    "        filename = (u.netloc + u.path).replace(\"/\",\"_\")\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(image_directory + filename, 'wb') as out_file:\n",
    "                shutil.copyfileobj(response.raw, out_file)\n",
    "            filenames.append(filename)\n",
    "        else:\n",
    "            print(\"Connection Error: {}\".format(url))\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "filename_df = df[df['contains_image_by_e'] == True].apply(download_images, axis=1)\n",
    "\n",
    "# I need a data frame like this as output from the download_images function,\n",
    "# which can then be merged with the main df.\n",
    "#\n",
    "# |-------|-------------------------------|\n",
    "# |       |  pbs.twimg.com_media_A6FV...  |\n",
    "# |       |-------------------------------|\n",
    "# |  76   |  pbs.twimg.com_media_A6Fr...  |\n",
    "# |       |-------------------------------|\n",
    "# |       |  pbs.twimg.com_media_A6Fq...  |\n",
    "# |-------|-------------------------------|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Output to Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Format to ImagePlot Macro for ImageJ:\n",
    "Data file that references the image filename (in one or more directories)\n",
    "\n",
    "####Data File Format\n",
    "File Type: Tab Delimited Text File  \n",
    "Cannot contain commas  \n",
    "Line 1: Headers  \n",
    "Line 2+: One line per image  \n",
    "\n",
    "Headers:\n",
    "* Must have at least 2 columns\n",
    "* Optional - filename or full file path\n",
    "* X & Y can only be integers or floating points\n",
    "* Text columns can be labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df['contains_image'] = (df['contains_image_by_e'] | df['contains_image_by_ee'] | df['contains_image_by_ex'] | df['contains_image_by_ct'])\n",
    "df['contains_image'] = (df['contains_image_by_e'] | df['contains_image_by_ex'] | df['contains_image_by_ct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.get_value(76,'e_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_tsv(df_selection):\n",
    "    \n",
    "    df_selection.set_index(['e_files'], inplace=True)\n",
    "    df_selection.index.name = None\n",
    "    return df_selection\n",
    "    \n",
    "    #for r in df.iteritems():\n",
    "    #    print(r)\n",
    "    \n",
    "    #for file_list in df['e_files']:\n",
    "    #    for file in file_list:\n",
    "    #        print(file)\n",
    "\n",
    "output_tsv(df.ix[df['contains_image'] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Output headers to tsv file.\n",
    "#Output (for now) basic fields like user ID, tweet ID, and others that are easy to output and are int/float per filename.\n",
    "#Each image filename will be one line even if this duplicates tweets (it will)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
